{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d4bb6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7652a4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0375d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../IMDB_dataset/IMDB dataset.csv')\n",
    "\n",
    "df.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
    "df.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
    "df = df.rename(columns={\"review\": \"text\", \"sentiment\":\"label\"}) \n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88a514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(train, preserve_index=False)\n",
    "dataset_test = Dataset.from_pandas(test, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65c58d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f02b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"I did not watch the entire movie. I could not watch the entire movie. I stopped the DVD after watching for half an hour and I suggest anyone thinking of watching themselves it stop themselves before taking the disc out of the case.<br /><br />I like Mafia movies both tragic and comic but Corky Romano can only be described as a tragic attempt at a mafia comedy.<br /><br />The problem is Corky Romano simply tries too hard to get the audience to laugh, the plot seems to be an excuse for moving Chris Kattan (Corky) from one scene to another. Corky himself is completely overplayed and lacks subtlety or credulity - all his strange mannerisms come across as contrived - Chris Kattan is clearly 'acting' rather than taking a role - it bounces you right out of the story. Each scene is utterly predictable, the 'comedic event' that will occur on the set is obvious as soon as each scene is introduced. In comedies such as Mr. Bean the disasters caused by the title character are funny because you can empathise with the characters motivations and initial event and the situation the character ends up in is not telegraphed. Corky however gives the feeling that he is deliberately screwing up in a desperate attempt to draw a laugh from the audience.<br /><br />If Chris had not played such an alien character (who never really connects with the other characters in the movie) and whose behaviour is entirely inexplicable (except for trying to draw laughs) and the comedy scenes weren't so predictable and stereotyped - all the jokes seemed far too familiar) this movie could have been watchable. But it isn't. Don't watch it.\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3745ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2a6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f19fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "dataset_train_tokenized = dataset_train.map(tokenize, batched=True)\n",
    "dataset_test_tokenized = dataset_test.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca93e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc695fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe1d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = dataset_train_tokenized.shuffle(seed=42).select(range(25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02ad214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c69c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c561ca",
   "metadata": {},
   "source": [
    "axis = -1\n",
    "\n",
    "This means that the index that will be returned by argmax will be taken from the last axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f55b3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0adc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"bert\", \n",
    "                                  evaluation_strategy = \"epoch\",\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                  per_device_train_batch_size = 4,\n",
    "                                  per_device_eval_batch_size = 4,\n",
    "                                  num_train_epochs = 2,\n",
    "                                  learning_rate = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec32974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = dataset_test_tokenized,\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b12880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojte\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 2:06:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>0.363978</td>\n",
       "      <td>0.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.351262</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.2772796936035156, metrics={'train_runtime': 7582.8893, 'train_samples_per_second': 6.594, 'train_steps_per_second': 1.648, 'total_flos': 6623369932800000.0, 'train_loss': 0.2772796936035156, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8038df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "793b0cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert/tokenizer/tokenizer_config.json',\n",
       " './bert/tokenizer/special_tokens_map.json',\n",
       " './bert/tokenizer/vocab.txt',\n",
       " './bert/tokenizer/added_tokens.json',\n",
       " './bert/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./bert/tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53198dae",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b6270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wojte\\anaconda3\\envs\\reviews-analysis-api\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6198e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert_folder = './bert/'\n",
    "bert_tokenizer_folder = './bert/tokenizer/'\n",
    "\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(model_bert_folder)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_tokenizer_folder)\n",
    "\n",
    "bert_pipe = pipeline(\"text-classification\", model=model_bert, tokenizer=bert_tokenizer, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560c89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"Do not look at this through the prism of \"Foreign Films\". You'd be wasting your time and miss something far too important.\n",
    "\n",
    "Hollywood does scale like nobody else, leaving the competition gasping in its wake. France does intimacy, and brutality. Nothing is sacred. And rather than try to revive the New Wave or emulate Hollywood like most widely seen French films of late, \"Intouchables\" harnesses its core strengths - ease with intimacy, willingness to ridicule anything and brutal honesty - and delivers one of the funniest, most honest and touching films I have ever seen.\n",
    "\n",
    "Sy is a failed robber, going through the motions and playing the stereotypical jobless émigré. Cluzet is a romantic and melancholy mind trapped in a useless body. The circumstances that bring them together are too funny to spoil here, but meet they do, and an awkward relationship quickly blossoms as they bring out the best in each other.\n",
    "\n",
    "The film's simplicity is delightfully misleading: the script is a masterpiece of comedy writing, and however good the rest of the cast is, the central duo is magical. Sy's comic timing will have you in stitches, but it is his honesty and vulnerability that make you fall in love with the character. Cluzet isn't your typical sad-sack, instead, much of the finest pleasures in the film consist in watching him use his keen mind to mess with the world around him (a subplot about an abstract painting really takes the biscuit, you'll know it when you see it).\n",
    "\n",
    "This is one of the most unique, beautiful and honest friendships ever committed to film. It will make you laugh, it will make you cry... a delightful celebration of everything in life that makes it worthwhile.\"\"\"\n",
    "\n",
    "review2 = \"\"\"This movie is an absolute slog. It did a terrible job of keeping me interested and invested. It doesn't take much to hook me, but I was barely engaged. I felt this from beginning to end.\n",
    "\n",
    "So much of this movie feels off. It's filled with eyebrow-raising moments. Like really odd character reaction shots and super awkward editing. Or other weird things, like a character who is listening, so she repeatedly says, \"Listening.\" I couldn't help but laugh.\n",
    "\n",
    "The only thing keeping me from leaving the theater was finding out the reveal and explanation. Unfortunately it's super lame and kind of dumb. Also, I'm a fan of Tina Fey but she doesn't fit here.\n",
    "\n",
    "The one positive that stands out to me is it's visually appealing. That's about it. I had a terrible time with this movie. Knives Out is now the clear winner of this mystery movie series rivalry.\"\"\"\n",
    "review3 = \"\"\"This movie is an absolute slog. It did a terrible job of keeping me interested and invested. It doesn't take much to hook me, but I was barely engaged. I felt this from beginning to end.\n",
    "\n",
    "So much of this movie feels off. It's filled with eyebrow-raising moments. Like really odd character reaction shots and super awkward editing. Or other weird things, like a character who is listening, so she repeatedly says, \"Listening.\" I couldn't help but laugh.\n",
    "\n",
    "The only thing keeping me from leaving the theater was finding out the reveal and explanation. Unfortunately it's super lame and kind of dumb. Also, I'm a fan of Tina Fey but she doesn't fit here.\n",
    "\n",
    "The one positive that stands out to me is it's visually appealing. That's about it. I had a terrible time with this movie. Knives Out is now the clear winner of this mystery movie series rivalry.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e848975",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bert_pipe([review, review2, review3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bb9d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_1', 'score': 0.9996761083602905},\n",
       "  {'label': 'LABEL_0', 'score': 0.000323950604069978}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9996267557144165},\n",
       "  {'label': 'LABEL_1', 'score': 0.00037324457662180066}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9996267557144165},\n",
       "  {'label': 'LABEL_1', 'score': 0.00037324457662180066}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58243ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prob_pos': '1.00', 'prob_neg': '0.00'}, {'prob_pos': '0.00', 'prob_neg': '1.00'}, {'prob_pos': '0.00', 'prob_neg': '1.00'}]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for prediction in predictions:\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for score in prediction:\n",
    "        if score['label'] == 'LABEL_1':\n",
    "            pos = score['score']\n",
    "        elif score['label'] == 'LABEL_0':\n",
    "            neg = score['score']\n",
    "    output.append({\"prob_pos\": \"{:.2f}\".format(float(pos)), \"prob_neg\": \"{:.2f}\".format(float(neg))})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69006171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ba48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('../IMDB_dataset/IMDB dataset.csv')['review'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ea2455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    One of the other reviewers has mentioned that ...\n",
       "1    A wonderful little production. <br /><br />The...\n",
       "2    I thought this was a wonderful way to spend ti...\n",
       "3    Basically there's a family where a little boy ...\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
       "5    Probably my all-time favorite movie, a story o...\n",
       "6    I sure would like to see a resurrection of a u...\n",
       "7    This show was an amazing, fresh & innovative i...\n",
       "8    Encouraged by the positive comments about this...\n",
       "9    If you like original gut wrenching laughter yo...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16449bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = bert_pipe([review for review in df_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa10d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_1', 'score': 0.9988672733306885},\n",
       "  {'label': 'LABEL_0', 'score': 0.001132776727899909}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9996291399002075},\n",
       "  {'label': 'LABEL_0', 'score': 0.00037088527460582554}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9991717338562012},\n",
       "  {'label': 'LABEL_0', 'score': 0.0008282645721919835}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9995326995849609},\n",
       "  {'label': 'LABEL_1', 'score': 0.0004672856885008514}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9996320009231567},\n",
       "  {'label': 'LABEL_0', 'score': 0.00036793045001104474}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9995561242103577},\n",
       "  {'label': 'LABEL_0', 'score': 0.0004438576870597899}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9815404415130615},\n",
       "  {'label': 'LABEL_0', 'score': 0.018459510058164597}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9996428489685059},\n",
       "  {'label': 'LABEL_1', 'score': 0.0003571833949536085}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9997969269752502},\n",
       "  {'label': 'LABEL_1', 'score': 0.00020313005370553583}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9994538426399231},\n",
       "  {'label': 'LABEL_0', 'score': 0.0005461304681375623}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b4714fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for prediction in predictions:\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for score in prediction:\n",
    "        if score['label'] == 'LABEL_1':\n",
    "            pos = score['score']\n",
    "        elif score['label'] == 'LABEL_0':\n",
    "            neg = score['score']\n",
    "    output.append({\"prob_pos\": \"{:.2f}\".format(float(pos)), \"prob_neg\": \"{:.2f}\".format(float(neg))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89925db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prob_pos': '1.00', 'prob_neg': '0.00'},\n",
       " {'prob_pos': '1.00', 'prob_neg': '0.00'},\n",
       " {'prob_pos': '1.00', 'prob_neg': '0.00'},\n",
       " {'prob_pos': '0.00', 'prob_neg': '1.00'},\n",
       " {'prob_pos': '1.00', 'prob_neg': '0.00'},\n",
       " {'prob_pos': '1.00', 'prob_neg': '0.00'},\n",
       " {'prob_pos': '0.98', 'prob_neg': '0.02'},\n",
       " {'prob_pos': '0.00', 'prob_neg': '1.00'},\n",
       " {'prob_pos': '0.00', 'prob_neg': '1.00'},\n",
       " {'prob_pos': '1.00', 'prob_neg': '0.00'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8f58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "for sentiment in output:\n",
    "    if float(sentiment['prob_pos']) > 0.5:\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01dd5966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aea17b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e11ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
